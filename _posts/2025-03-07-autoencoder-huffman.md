---
title: Exciting News, My Paper is Published!
author: Gajendra Saraswat
layout: post
---

ğŸŒŸ Exciting News: Autoencoder-Huffman Ensemble Model for Image Compression paper is now Published! ğŸŒŸ

Iâ€™m thrilled to share that our paper, â€œğ—”ğ˜‚ğ˜ğ—¼ğ—²ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿-ğ—›ğ˜‚ğ—³ğ—³ğ—ºğ—®ğ—» ğ—˜ğ—»ğ˜€ğ—²ğ—ºğ—¯ğ—¹ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—³ğ—¼ğ—¿ ğ—œğ—ºğ—®ğ—´ğ—² ğ—–ğ—¼ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»â€, has been published! ğŸ‰ This work merges two powerful worlds of deep learning and traditional encoding to push the boundaries of efficient image compression.

ğŸ“– Whatâ€™s the research about?
Autoencoders are fantastic at distilling data into a compact Latent Space Representation (LSR) while filtering out noise. But we wanted to go further: by pairing an autoencoder with a Huffman encoder, we created a two-stage compression model. First, the autoencoder condenses images into their core LSR, and then Huffman encoding squeezes it even further. The result? Minimal information loss and impressive compression ratios across diverse datasets like MNIST, CIFAR-10, and Fashion MNIST.

ğŸ” Key Findings:
Achieved Average Compression Ratios (ACRs) of 15.654 (MNIST), 24.230 (CIFAR-10), and 15.171 (Fashion MNIST) without overhead.
Even with Huffman code overhead, ACRs remained strong at ~5.5â€“8.7, proving the modelâ€™s robustness across varying latent space sizes (70â€“100 neurons).
This work underscores how blending deep learning with classical techniques can unlock new efficiencies in data storage and transmission.

ğŸ™ Thank You!
None of this would have been possible without the incredible support of so many:
My mentors and collaborators, for challenging my ideas and guiding me through countless revisions. Thank you to the ever amazing Prof. U.S.N. Raju for leading us all the way. Thanks Lakshmi vara prasad Bommalata chilakala shanthi and Ala Harika for your continued support throughout the journey.
My brother Bharat Saraswat, Priyanka Saraswat and my family, for being my rock and loudest cheerleader.

Reviewers and editors, for sharpening this work into its final form.
Friends and family, for your patience during those late-night coding sessions.
This milestone isnâ€™t just mine, itâ€™s a testament to the power of collaboration. ğŸ’™

ğŸ”— Dive into the full paper here: https://lnkd.in/gPfwmq9T
A huge thanks to Springer Nature for publishing this work and supporting open science!

Last but not least, thanks to NIT Warangal for giving me this chance, I will forever be grateful to the memories and successes I collected there.

Hereâ€™s to more innovations at the intersection of AI and traditional computing! ğŸš€
hashtag#DeepLearning hashtag#ImageCompression hashtag#Research hashtag#AI hashtag#Gratitude